{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir --ignore-installed numpy pandas bs4 nltk scikit-learn matplotlib graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#adapted from:\n",
    "#https://github.com/sahilee26/IMDB-Movie-Reviews-Sentiment-Analysis/blob/master/Bag-of-words-random-forest.ipynb\n",
    "#https://github.com/shiaoligreen/practical-data-science/tree/master/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn\n",
    "\n",
    "# Load packages\n",
    "\n",
    "#Generic tools\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#Data pre-preprocessing\n",
    "import pandas as pd  \n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltkstopwords\n",
    "import nltk.data\n",
    "\n",
    "#Data split and featurization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Random forest classifier and evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Random forest visualization\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#Multilayer Perceptron classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", \n",
    "                    header=0, delimiter=\"\\t\", \n",
    "                    quoting=3)\n",
    "\n",
    "test = pd.read_csv(\"data/testData.tsv\", \n",
    "                   header=0, delimiter=\"\\t\",\n",
    "                   quoting=3 )\n",
    "\n",
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, \n",
    "                              delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Split data to train and test partitions\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(columns=['sentiment']), \n",
    "                                                    train.sentiment, test_size=0.2)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xiuwen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download text datasets, including stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltkstopwords\n",
    "#print(nltkstopwords.words('english'))\n",
    "#print(nltkstopwords.words('german'))\n",
    "#print(nltkstopwords.words('chinese'))\n",
    "#print(nltkstopwords.words.__dir__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def review_to_words(review, string=True, remove_stopwords=False):\n",
    "    '''\n",
    "    Function to convert a document to a sequence of words, \n",
    "    optionally removing stop words.  \n",
    "    Returns a list of words.\n",
    "    '''\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(nltkstopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    if string:\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START i', 'i liked', 'liked this', 'this movie', 'movie END']\n"
     ]
    }
   ],
   "source": [
    "def review_to_bigrams(review, remove_stopwords=False, add_startend_tokens=True):\n",
    "    '''\n",
    "    Function to convert a document to a sequence of word bigrams,\n",
    "    optionally removing stop words.\n",
    "    Returns a list of bigrams.\n",
    "    '''\n",
    "    #E.g., [\"I\", \"liked\", \"this\" ,\"movie\"] -> [\"I liked\", \"liked this\", \"this movie\"]\n",
    "    #your code here     \n",
    "    \n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # Optionally add START and END tokens (True by default)\n",
    "    if add_startend_tokens:\n",
    "        words = [\"START\"] + words + [\"END\"]\n",
    "        \n",
    "    # Optionally remove stop words (False by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(nltkstopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Generate bigrams\n",
    "    bigrams = []\n",
    "    for i in range(len(words)-1):\n",
    "        bigrams.append(words[i] + \" \" + words[i+1])\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "#print(review_to_bigrams([\"I\", \"liked\", \"this\", \"movie\"]))\n",
    "my_bigrams =review_to_bigrams(\"I liked this movie\")\n",
    "print(my_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiuwen\\AppData\\Local\\Temp\\ipykernel_14340\\766223517.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    }
   ],
   "source": [
    "# Get list of reviews\n",
    "clean_train_reviews = [review_to_words(X_train[\"review\"][i], remove_stopwords=True) for i in range(len(X_train))]\n",
    "clean_test_reviews = [review_to_words(X_test[\"review\"][i], remove_stopwords=True) for i in range(len(X_test))]\n",
    "\n",
    "print(clean_train_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiuwen\\AppData\\Local\\Temp\\ipykernel_10872\\885493429.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START this', 'this is', 'is one', 'one of', 'of my', 'my favorite', 'favorite comedies', 'comedies ever', 'ever not', 'not wanting', 'wanting to', 'to condone', 'condone the', 'the uninspiring', 'uninspiring lifestyle', 'lifestyle of', 'of its', 'its hero', 'hero but', 'but taken', 'taken for', 'for what', 'what it', 'it s', 's worth', 'worth and', 'and not', 'not as', 'as trivializing', 'trivializing alcoholism', 'alcoholism the', 'the movie', 'movie is', 'is simply', 'simply a', 'a lot', 'lot of', 'of fun', 'fun it', 'it tells', 'tells the', 'the unlikely', 'unlikely tale', 'tale of', 'of a', 'a perpetually', 'perpetually drunk', 'drunk irresponsible', 'irresponsible something', 'something bachelor', 'bachelor named', 'named arthur', 'arthur who', 'who is', 'is set', 'set to', 'to inherit', 'inherit a', 'a vast', 'vast fortune', 'fortune but', 'but only', 'only if', 'if he', 'he marries', 'marries susan', 'susan chosen', 'chosen because', 'because the', 'the family', 'family thinks', 'thinks she', 'she might', 'might make', 'make something', 'something of', 'of him', 'him arthur', 'arthur proposes', 'proposes but', 'but then', 'then unwisely', 'unwisely falls', 'falls for', 'for linda', 'linda a', 'a waitress', 'waitress and', 'and petty', 'petty thief', 'thief dudley', 'dudley moore', 'moore is', 'is perfect', 'perfect as', 'as arthur', 'arthur the', 'the world', 'world s', 's most', 'most endearing', 'endearing drunk', 'drunk whose', 'whose antics', 'antics are', 'are a', 'a laugh', 'laugh a', 'a minute', 'minute admittedly', 'admittedly moore', 'moore is', 'is arthur', 'arthur and', 'and i', 'i agree', 'agree with', 'with those', 'those who', 'who can', 'can imagine', 'imagine no', 'no other', 'other actor', 'actor in', 'in the', 'the role', 'role the', 'the ladies', 'ladies of', 'of the', 'the piece', 'piece are', 'are also', 'also well', 'well portrayed', 'portrayed liza', 'liza minnelli', 'minnelli sparkles', 'sparkles as', 'as linda', 'linda and', 'and her', 'her on', 'on screen', 'screen chemistry', 'chemistry with', 'with moore', 'moore is', 'is great', 'great jill', 'jill clayburgh', 'clayburgh plays', 'plays susan', 'susan the', 'the wealthy', 'wealthy and', 'and more', 'more appropriate', 'appropriate woman', 'woman chosen', 'chosen for', 'for arthur', 'arthur however', 'however this', 'this film', 'film is', 'is literally', 'literally made', 'made by', 'by sir', 'sir john', 'john gielgud', 'gielgud who', 'who portrays', 'portrays arthur', 'arthur s', 's sarcastic', 'sarcastic but', 'but moral', 'moral butler', 'butler hobson', 'hobson it', 'it s', 's obvious', 'obvious these', 'these two', 'two have', 'have had', 'had a', 'a great', 'great mutual', 'mutual affection', 'affection during', 'during hobson', 'hobson s', 's longtime', 'longtime employment', 'employment hobson', 'hobson is', 'is arthur', 'arthur s', 's best', 'best friend', 'friend and', 'and purveyor', 'purveyor of', 'of unsolicited', 'unsolicited commentary', 'commentary and', 'and advice', 'advice the', 'the most', 'most interesting', 'interesting relationship', 'relationship in', 'in this', 'this film', 'film is', 'is not', 'not arthur', 'arthur s', 's romance', 'romance at', 'at all', 'all but', 'but his', 'his unusual', 'unusual rapport', 'rapport with', 'with this', 'this witty', 'witty and', 'and of', 'of course', 'course perpetually', 'perpetually disapproving', 'disapproving servant', 'servant it', 'it s', 's the', 'the butler', 'butler you', 'you ll', 'll remember', 'remember best', 'best long', 'long after', 'after the', 'the closing', 'closing credits', 'credits roll', 'roll END']\n"
     ]
    }
   ],
   "source": [
    "# Get lists of reviews using the bigram function instead of the review_to_words function\n",
    "\n",
    "clean_train_bigram_reviews = [review_to_bigrams(X_train[\"review\"][i]) for i in range(len(X_train))]\n",
    "clean_test_bigram_reviews = [review_to_bigrams(X_test[\"review\"][i]) for i in range(len(X_test))]\n",
    "\n",
    "print(clean_train_bigram_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a bag of words  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) #vocabulary size defined here, sorted by frequency - e.g., 5k most common terms.  How does model performance change if you increase/decrease this value?\n",
    "\n",
    "# Fit transform the data\n",
    "train_feat = vectorizer.fit_transform(clean_train_reviews).toarray()\n",
    "test_feat = vectorizer.transform(clean_test_reviews).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned' 'abc' 'abilities' 'ability' 'able' 'abraham' 'absence'\n",
      " 'absolute' 'absolutely' 'absurd' 'abuse' 'abusive' 'abysmal' 'academy'\n",
      " 'accent' 'accents' 'accept' 'acceptable' 'accepted' 'access' 'accident'\n",
      " 'accidentally' 'accompanied' 'accomplished' 'according' 'account'\n",
      " 'accuracy' 'accurate' 'accused' 'achieve' 'achieved' 'achievement' 'acid'\n",
      " 'across' 'act' 'acted' 'acting' 'action' 'actions' 'activities' 'actor'\n",
      " 'actors' 'actress' 'actresses' 'acts' 'actual' 'actually' 'ad' 'adam'\n",
      " 'adams' 'adaptation' 'adaptations' 'adapted' 'add' 'added' 'addicted'\n",
      " 'adding' 'addition' 'additional' 'adds' 'adequate' 'admire' 'admit'\n",
      " 'admittedly' 'adolescent' 'adorable' 'adult' 'adults' 'advance'\n",
      " 'advanced' 'advantage' 'adventure' 'adventures' 'advertising' 'advice'\n",
      " 'advise' 'affair' 'affect' 'affected' 'afford' 'aforementioned' 'afraid'\n",
      " 'africa' 'african' 'afternoon' 'afterwards' 'age' 'aged' 'agenda' 'agent'\n",
      " 'agents' 'ages' 'aging' 'ago' 'agree' 'agreed' 'agrees' 'ah' 'ahead'\n",
      " 'aid']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "def get_preds(test_feat, train_feat, y_test, y_train, model, title='Random Forest'):\n",
    "    print(\"Training model, this may take some time...\")\n",
    "    model.fit(train_feat, y_train)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    y_preds = model.predict(test_feat)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    #fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "    #roc_auc = auc(fpr, tpr)\n",
    "    #print('AUC:', roc_auc)\n",
    "    \n",
    "    #F1 doesn't matter because of class balance here\n",
    "    \n",
    "    # plot AUC\n",
    "    #plt.plot(fpr, tpr)\n",
    "    #plt.title(title)\n",
    "    #plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    #plt.xlabel('False Positive Rate')\n",
    "    #plt.ylabel('True Positive Rate')\n",
    "    #plt.show()\n",
    "    \n",
    "    return y_preds, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, this may take some time...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8476\n"
     ]
    }
   ],
   "source": [
    "preds_rf, model = get_preds(test_feat, train_feat, \n",
    "                  y_test, y_train, \n",
    "                  RandomForestClassifier(n_estimators = 100)) #How does performance changes if you increase/decrease the number of estimators (trees)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, this may take some time...\n",
      "Evaluating model...\n",
      "Accuracy: 0.8532\n"
     ]
    }
   ],
   "source": [
    "preds_rf, model = get_preds(test_feat, train_feat, \n",
    "                  y_test, y_train, \n",
    "                  LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf, model = get_preds(test_feat, train_feat, \n",
    "                  y_test, y_train, \n",
    "                  LogisticRegression(penalty='l2', max_iter=500, C=0.1, random_state=42)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf, model = get_preds(test_feat, train_feat, \n",
    "                  y_test, y_train, \n",
    "                  LogisticRegression(penalty='l2', max_iter=500, C=1, class_weight='balanced', random_state=42)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf, model = get_preds(test_feat, train_feat, \n",
    "                  y_test, y_train, \n",
    "                  LogisticRegression(penalty='l2', max_iter=500, C=1, class_weight='balanced', random_state=42)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a list of models with different parameters\n",
    "models = [\n",
    "    LogisticRegression(penalty='l2', max_iter=500, C=0.1, random_state=42),\n",
    "    LogisticRegression(penalty='l2', max_iter=500, C=1, solver='liblinear', random_state=42),\n",
    "    LogisticRegression(penalty='l2', max_iter=500, C=1, class_weight='balanced', random_state=42),\n",
    "    LogisticRegression(penalty='l2', max_iter=1000, C=0.5, solver='saga', class_weight='balanced', random_state=42)\n",
    "]\n",
    "\n",
    "# Initialize variables to keep track of the best model and its score\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "# Iterate through the list of models\n",
    "for model in models:\n",
    "    # Fit the model on the training data\n",
    "    model.fit(train_feat, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    preds = model.predict(test_feat)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    score = accuracy_score(y_test, preds)\n",
    "\n",
    "    print(f'Model: {model}')\n",
    "    print(f'Accuracy: {score}')\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if score > best_score:\n",
    "        best_model = model\n",
    "        best_score = score\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Output the best model and its score\n",
    "print(f'Best Model: {best_model}')\n",
    "print(f'Best Accuracy: {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf, model = get_preds(test_feat, train_feat, \n",
    "                  y_test, y_train, \n",
    "                  SGDClassifier(loss='hinge',max_iter=500,random_state=42)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # visualize decision tree from classifier\n",
    "for i in range(10):\n",
    "    tree = model.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=vocab,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(preds_rf[0]) #first prediction for test item - model predicted positive sentiment\n",
    "print(test_feat[0]) #first test review\n",
    "print(clean_test_reviews[0])\n",
    "print(type(y_test))\n",
    "print(y_test.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "print(\"Training MLP classifier... this may take some time\")\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,),random_state=1, max_iter=300, verbose=True).fit(train_feat, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = mlp_clf.score(test_feat, y_test)\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "print(\"Training deeper MLP classifier... this may take some time\")\n",
    "mlp_clf_deeper = MLPClassifier(hidden_layer_sizes=(500,250,500,),random_state=1, max_iter=300, verbose=True).fit(train_feat, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = mlp_clf_deeper.score(test_feat, y_test)\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "print(\"Training deeper MLP classifier... this may take some time\")\n",
    "mlp_clf_deeper = MLPClassifier(hidden_layer_sizes=(500,250,500,),random_state=1, max_iter=300, verbose=True).fit(train_feat, y_train)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
